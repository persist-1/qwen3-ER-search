#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡æ•°æ®åº“å¯è§†åŒ–è½¯ä»¶
åŸºäºStreamlitçš„Webç•Œé¢ï¼Œç”¨äºæŸ¥çœ‹å’Œæ“ä½œChromaå‘é‡æ•°æ®åº“
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import chromadb
from chromadb.config import Settings
import json
import os
from datetime import datetime
import numpy as np
from typing import Dict, List, Optional
import time

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="å‘é‡æ•°æ®åº“å¯è§†åŒ–å·¥å…·",
    page_icon="ğŸ—„ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ·»åŠ æµè§ˆå™¨å…¼å®¹æ€§ä¿®å¤çš„CSSå’ŒJavaScript
st.markdown("""
<style>
/* ä¿®å¤æµè§ˆå™¨å…¼å®¹æ€§é—®é¢˜ */
@supports not (Object.hasOwn) {
    .stMarkdown {
        word-break: break-word;
    }
}

/* ç¡®ä¿æ‰€æœ‰æ–‡æœ¬åŒºåŸŸéƒ½æœ‰é€‚å½“çš„æ ·å¼ */
.stTextArea textarea {
    font-family: monospace;
    font-size: 12px;
}
</style>

<script>
// ä¿®å¤ Object.hasOwn å…¼å®¹æ€§é—®é¢˜
if (!Object.hasOwn) {
    Object.hasOwn = function(obj, prop) {
        return Object.prototype.hasOwnProperty.call(obj, prop);
    };
}
</script>
""", unsafe_allow_html=True)

class VectorDBViewer:
    def __init__(self, db_path: str = "vector_db", collection_name: str = "documents"):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“æŸ¥çœ‹å™¨
        Args:
            db_path: æ•°æ®åº“è·¯å¾„
            collection_name: é›†åˆåç§°
        """
        self.db_path = db_path
        self.collection_name = collection_name
        self.client = None
        self.collection = None
        self.connect_database()
    
    def connect_database(self):
        """è¿æ¥åˆ°å‘é‡æ•°æ®åº“"""
        try:
            self.client = chromadb.PersistentClient(
                path=self.db_path,
                settings=Settings(anonymized_telemetry=False)
            )
            self.collection = self.client.get_collection(name=self.collection_name)
            return True
        except Exception as e:
            st.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def get_database_stats(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            count = self.collection.count()
            all_results = self.collection.get()
            
            # ç»Ÿè®¡æ–‡æ¡£ID
            document_ids = set()
            for metadata in all_results['metadatas']:
                if metadata and 'document_id' in metadata:
                    document_ids.add(metadata['document_id'])
            
            # è®¡ç®—æ•°æ®åº“å¤§å°
            db_size = 0
            for root, dirs, files in os.walk(self.db_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    db_size += os.path.getsize(file_path)
            
            stats = {
                'total_chunks': count,
                'unique_documents': len(document_ids),
                'document_ids': list(document_ids),
                'database_size_mb': db_size / (1024 * 1024),
                'collection_name': self.collection_name,
                'database_path': self.db_path
            }
            
            return stats
        except Exception as e:
            st.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def get_all_documents(self) -> pd.DataFrame:
        """è·å–æ‰€æœ‰æ–‡æ¡£æ•°æ®"""
        try:
            results = self.collection.get()
            
            # åˆ›å»ºDataFrame
            data = []
            for i in range(len(results['ids'])):
                metadata = results['metadatas'][i] or {}
                data.append({
                    'ID': results['ids'][i],
                    'Document': results['documents'][i],
                    'Document_ID': metadata.get('document_id', ''),
                    'Chunk_Index': metadata.get('chunk_index', ''),
                    'Chunk_Length': metadata.get('chunk_length', ''),
                    'Source': metadata.get('source', ''),
                    'Category': metadata.get('category', ''),
                    'Language': metadata.get('language', ''),
                    'Timestamp': metadata.get('timestamp', ''),
                    'File_Name': metadata.get('file_name', ''),
                    'File_Size': metadata.get('file_size', '')
                })
            
            return pd.DataFrame(data)
        except Exception as e:
            st.error(f"è·å–æ–‡æ¡£æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def search_documents(self, query: str, top_k: int = 10) -> List[Dict]:
        """æœç´¢æ–‡æ¡£"""
        try:
            # è¿™é‡Œéœ€è¦åŠ è½½embeddingæ¨¡å‹æ¥ç”ŸæˆæŸ¥è¯¢å‘é‡
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬æœç´¢
            all_docs = self.get_all_documents()
            
            # ç®€å•çš„æ–‡æœ¬æœç´¢
            results = []
            for _, row in all_docs.iterrows():
                if query.lower() in row['Document'].lower():
                    results.append({
                        'ID': row['ID'],
                        'Document': row['Document'],
                        'Document_ID': row['Document_ID'],
                        'Chunk_Index': row['Chunk_Index'],
                        'Source': row['Source'],
                        'Category': row['Category']
                    })
            
            return results[:top_k]
        except Exception as e:
            st.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def delete_document(self, document_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å±äºè¯¥æ–‡æ¡£çš„å—
            results = self.collection.get(where={"document_id": document_id})
            
            if not results['ids']:
                st.warning(f"æœªæ‰¾åˆ°æ–‡æ¡£IDä¸º {document_id} çš„æ–‡æ¡£")
                return False
            
            # åˆ é™¤è¿™äº›å—
            self.collection.delete(ids=results['ids'])
            st.success(f"æˆåŠŸåˆ é™¤æ–‡æ¡£ {document_id} çš„ {len(results['ids'])} ä¸ªå—")
            return True
            
        except Exception as e:
            st.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def add_document(self, document_text: str, document_id: str, metadata: Dict = None) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°æ•°æ®åº“"""
        try:
            if not metadata:
                metadata = {
                    "source": "web_upload",
                    "category": "general",
                    "language": "zh",
                    "timestamp": datetime.now().isoformat(),
                    "file_name": f"{document_id}.txt"
                }
            
            # åˆ†å‰²æ–‡æœ¬ä¸ºå—
            chunks = self._split_text(document_text)
            
            # ä½¿ç”¨Qwen3-Embeddingæ¨¡å‹ç”Ÿæˆ1024ç»´å‘é‡
            import sys
            import os
            sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src', 'core'))
            
            try:
                from test_qwen3_embedding import Qwen3Embedding
                import torch
                
                # åˆå§‹åŒ–embeddingæ¨¡å‹
                embedding_model = Qwen3Embedding("models/Qwen3-Embedding-0.6B/Qwen/Qwen3-Embedding-0.6B")
                
                # ç”Ÿæˆå‘é‡
                with torch.inference_mode():
                    embeddings = embedding_model.encode(chunks, is_query=False)
                
                # æ·»åŠ åˆ°æ•°æ®åº“
                self.collection.add(
                    documents=chunks,
                    embeddings=embeddings.cpu().numpy().tolist(),
                    metadatas=[{
                        **metadata,
                        "document_id": document_id,
                        "chunk_index": i,
                        "chunk_length": len(chunk)
                    } for i, chunk in enumerate(chunks)],
                    ids=[f"{document_id}_chunk_{i}" for i in range(len(chunks))]
                )
                
                return True
                
            except ImportError:
                # å¦‚æœæ— æ³•å¯¼å…¥Qwen3æ¨¡å‹ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                st.warning("æ— æ³•åŠ è½½Qwen3-Embeddingæ¨¡å‹ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                return self._add_document_fallback(chunks, document_id, metadata)
                
        except Exception as e:
            st.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def _add_document_fallback(self, chunks: List[str], document_id: str, metadata: Dict) -> bool:
        """å¤‡ç”¨æ·»åŠ æ–¹æ¡ˆï¼šåˆ›å»ºæ–°çš„collection"""
        try:
            # åˆ›å»ºä¸€ä¸ªæ–°çš„collectionç”¨äºæµ‹è¯•
            test_collection_name = f"documents_{int(time.time())}"
            test_collection = self.client.create_collection(name=test_collection_name)
            
            # æ·»åŠ åˆ°æ–°collection
            test_collection.add(
                documents=chunks,
                metadatas=[{
                    **metadata,
                    "document_id": document_id,
                    "chunk_index": i,
                    "chunk_length": len(chunk)
                } for i, chunk in enumerate(chunks)],
                ids=[f"{document_id}_chunk_{i}" for i in range(len(chunks))]
            )
            
            st.success(f"æ–‡æ¡£å·²æ·»åŠ åˆ°æ–°collection: {test_collection_name}")
            return True
            
        except Exception as e:
            st.error(f"å¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e}")
            return False
    
    def _split_text(self, text: str, chunk_size: int = 300) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆå—"""
        import re
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) <= chunk_size:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

    def get_document_info(self, document_id: str) -> Optional[Dict]:
        """è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯"""
        try:
            results = self.collection.get(where={"document_id": document_id})
            
            if not results['ids']:
                return None
            
            total_length = 0
            for metadata in results['metadatas']:
                if metadata:
                    total_length += metadata.get('chunk_length', 0)
            
            return {
                'chunks': len(results['ids']),
                'total_length': total_length,
                'first_chunk': results['documents'][0][:100] + "..." if results['documents'] else ""
            }
        except Exception as e:
            st.error(f"è·å–æ–‡æ¡£ä¿¡æ¯å¤±è´¥: {e}")
            return None

def main():
    st.title("ğŸ—„ï¸ å‘é‡æ•°æ®åº“å¯è§†åŒ–å·¥å…·")
    st.markdown("---")
    
    # åˆå§‹åŒ–session_state
    if 'refresh_trigger' not in st.session_state:
        st.session_state.refresh_trigger = 0
    
    # ä¾§è¾¹æ é…ç½®
    st.sidebar.header("âš™ï¸ é…ç½®")
    db_path = st.sidebar.text_input("æ•°æ®åº“è·¯å¾„", value="vector_db")
    collection_name = st.sidebar.text_input("é›†åˆåç§°", value="documents")
    
    # åˆå§‹åŒ–æŸ¥çœ‹å™¨
    viewer = VectorDBViewer(db_path, collection_name)
    
    if viewer.collection is None:
        st.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    # è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
    stats = viewer.get_database_stats()
    
    # ä¸»ç•Œé¢
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š æ¦‚è§ˆ", 
        "ğŸ“‹ æ–‡æ¡£åˆ—è¡¨", 
        "ğŸ” æœç´¢", 
        "ğŸ“ˆ åˆ†æ", 
        "âš™ï¸ ç®¡ç†"
    ])
    
    with tab1:
        st.header("ğŸ“Š æ•°æ®åº“æ¦‚è§ˆ")
        
        # ç»Ÿè®¡å¡ç‰‡
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="æ€»æ–‡æ¡£å—æ•°",
                value=stats.get('total_chunks', 0),
                help="æ•°æ®åº“ä¸­å­˜å‚¨çš„æ–‡æ¡£å—æ€»æ•°"
            )
        
        with col2:
            st.metric(
                label="å”¯ä¸€æ–‡æ¡£æ•°",
                value=stats.get('unique_documents', 0),
                help="ä¸åŒçš„æ–‡æ¡£æ•°é‡"
            )
        
        with col3:
            st.metric(
                label="æ•°æ®åº“å¤§å°",
                value=f"{stats.get('database_size_mb', 0):.2f} MB",
                help="æ•°æ®åº“æ–‡ä»¶å ç”¨çš„ç£ç›˜ç©ºé—´"
            )
        
        with col4:
            st.metric(
                label="é›†åˆåç§°",
                value=stats.get('collection_name', ''),
                help="å½“å‰è¿æ¥çš„é›†åˆåç§°"
            )
        
        # æ–‡æ¡£IDåˆ—è¡¨
        st.subheader("ğŸ“„ æ–‡æ¡£åˆ—è¡¨")
        if stats.get('document_ids'):
            for doc_id in stats['document_ids']:
                st.write(f"â€¢ {doc_id}")
        else:
            st.info("æš‚æ— æ–‡æ¡£")
        
        # æ•°æ®åº“ä¿¡æ¯
        st.subheader("â„¹ï¸ æ•°æ®åº“ä¿¡æ¯")
        info_df = pd.DataFrame([
            {"å±æ€§": "æ•°æ®åº“è·¯å¾„", "å€¼": stats.get('database_path', '')},
            {"å±æ€§": "é›†åˆåç§°", "å€¼": stats.get('collection_name', '')},
            {"å±æ€§": "æ€»å—æ•°", "å€¼": stats.get('total_chunks', 0)},
            {"å±æ€§": "å”¯ä¸€æ–‡æ¡£æ•°", "å€¼": stats.get('unique_documents', 0)},
            {"å±æ€§": "æ•°æ®åº“å¤§å°", "å€¼": f"{stats.get('database_size_mb', 0):.2f} MB"},
        ])
        st.dataframe(info_df, use_container_width=True)
    
    with tab2:
        st.header("ğŸ“‹ æ–‡æ¡£åˆ—è¡¨")
        
        # è·å–æ‰€æœ‰æ–‡æ¡£
        df = viewer.get_all_documents()
        
        if not df.empty:
            # æœç´¢æ¡†
            search_term = st.text_input("ğŸ” æœç´¢æ–‡æ¡£å†…å®¹", placeholder="è¾“å…¥å…³é”®è¯...")
            if search_term:
                df = df[df['Document'].str.contains(search_term, case=False, na=False)]
            
            # æ˜¾ç¤ºæ–‡æ¡£æ•°é‡
            st.info(f"æ˜¾ç¤º {len(df)} ä¸ªæ–‡æ¡£å—")
            
            # åˆ†é¡µæ˜¾ç¤º
            page_size = st.selectbox("æ¯é¡µæ˜¾ç¤ºæ•°é‡", [10, 25, 50, 100])
            total_pages = (len(df) + page_size - 1) // page_size
            
            if total_pages > 1:
                page = st.selectbox("é€‰æ‹©é¡µé¢", range(1, total_pages + 1))
                start_idx = (page - 1) * page_size
                end_idx = start_idx + page_size
                page_df = df.iloc[start_idx:end_idx]
            else:
                page_df = df
            
            # æ˜¾ç¤ºæ–‡æ¡£
            for idx, row in page_df.iterrows():
                with st.expander(f"ğŸ“„ {row['ID']} - {row['Document'][:100]}..."):
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.write("**æ–‡æ¡£å†…å®¹:**")
                        st.text_area("æ–‡æ¡£å†…å®¹", value=row['Document'], height=150, key=f"doc_{idx}", label_visibility="collapsed")
                    
                    with col2:
                        st.write("**å…ƒæ•°æ®:**")
                        metadata = {
                            "æ–‡æ¡£ID": row['Document_ID'],
                            "å—ç´¢å¼•": row['Chunk_Index'],
                            "å—é•¿åº¦": row['Chunk_Length'],
                            "æ¥æº": row['Source'],
                            "åˆ†ç±»": row['Category'],
                            "è¯­è¨€": row['Language'],
                            "æ—¶é—´æˆ³": row['Timestamp'],
                            "æ–‡ä»¶å": row['File_Name']
                        }
                        
                        for key, value in metadata.items():
                            if pd.notna(value) and value != '':
                                st.write(f"**{key}:** {value}")
        else:
            st.warning("æš‚æ— æ–‡æ¡£æ•°æ®")
    
    with tab3:
        st.header("ğŸ” æœç´¢æ–‡æ¡£")
        
        # æœç´¢é€‰é¡¹
        col1, col2 = st.columns([3, 1])
        
        with col1:
            search_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯", placeholder="ä¾‹å¦‚ï¼šæœºå™¨å­¦ä¹ ã€å·¥ä½œç»éªŒ...")
        
        with col2:
            top_k = st.number_input("è¿”å›ç»“æœæ•°é‡", min_value=1, max_value=50, value=10)
        
        if st.button("ğŸ” æœç´¢", type="primary"):
            if search_query:
                with st.spinner("æ­£åœ¨æœç´¢..."):
                    results = viewer.search_documents(search_query, top_k)
                
                if results:
                    st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
                    
                    for i, result in enumerate(results, 1):
                        with st.expander(f"ç»“æœ {i}: {result['ID']}"):
                            st.write("**æ–‡æ¡£å†…å®¹:**")
                            st.text_area("æœç´¢ç»“æœ", value=result['Document'], height=100, key=f"search_{i}", label_visibility="collapsed")
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.write(f"**æ–‡æ¡£ID:** {result['Document_ID']}")
                            with col2:
                                st.write(f"**å—ç´¢å¼•:** {result['Chunk_Index']}")
                            with col3:
                                st.write(f"**åˆ†ç±»:** {result['Category']}")
                else:
                    st.info("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            else:
                st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯")
    
    with tab4:
        st.header("ğŸ“ˆ æ•°æ®åˆ†æ")
        
        df = viewer.get_all_documents()
        
        if not df.empty:
            # æ–‡æ¡£åˆ†å¸ƒåˆ†æ
            st.subheader("ğŸ“Š æ–‡æ¡£åˆ†å¸ƒ")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # æŒ‰æ¥æºåˆ†å¸ƒ
                if 'Source' in df.columns and not df['Source'].isna().all():
                    source_counts = df['Source'].value_counts()
                    fig_source = px.pie(
                        values=source_counts.values,
                        names=source_counts.index,
                        title="æŒ‰æ¥æºåˆ†å¸ƒ"
                    )
                    st.plotly_chart(fig_source, use_container_width=True)
            
            with col2:
                # æŒ‰åˆ†ç±»åˆ†å¸ƒ
                if 'Category' in df.columns and not df['Category'].isna().all():
                    category_counts = df['Category'].value_counts()
                    fig_category = px.pie(
                        values=category_counts.values,
                        names=category_counts.index,
                        title="æŒ‰åˆ†ç±»åˆ†å¸ƒ"
                    )
                    st.plotly_chart(fig_category, use_container_width=True)
            
            # å—é•¿åº¦åˆ†å¸ƒ
            st.subheader("ğŸ“ å—é•¿åº¦åˆ†å¸ƒ")
            if 'Chunk_Length' in df.columns and not df['Chunk_Length'].isna().all():
                chunk_lengths = pd.to_numeric(df['Chunk_Length'], errors='coerce')
                chunk_lengths = chunk_lengths.dropna()
                
                if len(chunk_lengths) > 0:
                    fig_length = px.histogram(
                        x=chunk_lengths,
                        title="æ–‡æ¡£å—é•¿åº¦åˆ†å¸ƒ",
                        labels={'x': 'å—é•¿åº¦', 'y': 'æ•°é‡'}
                    )
                    st.plotly_chart(fig_length, use_container_width=True)
                    
                    # ç»Ÿè®¡ä¿¡æ¯
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("å¹³å‡é•¿åº¦", f"{chunk_lengths.mean():.1f}")
                    with col2:
                        st.metric("æœ€å¤§é•¿åº¦", f"{chunk_lengths.max():.0f}")
                    with col3:
                        st.metric("æœ€å°é•¿åº¦", f"{chunk_lengths.min():.0f}")
                    with col4:
                        st.metric("æ ‡å‡†å·®", f"{chunk_lengths.std():.1f}")
            
            # æ—¶é—´åˆ†å¸ƒ
            st.subheader("â° æ—¶é—´åˆ†å¸ƒ")
            if 'Timestamp' in df.columns and not df['Timestamp'].isna().all():
                # è§£ææ—¶é—´æˆ³
                timestamps = []
                for ts in df['Timestamp']:
                    try:
                        if pd.notna(ts) and ts != '':
                            timestamps.append(pd.to_datetime(ts))
                    except:
                        continue
                
                if timestamps:
                    timestamps = pd.Series(timestamps)
                    fig_time = px.histogram(
                        x=timestamps,
                        title="æ–‡æ¡£æ·»åŠ æ—¶é—´åˆ†å¸ƒ",
                        labels={'x': 'æ—¶é—´', 'y': 'æ•°é‡'}
                    )
                    st.plotly_chart(fig_time, use_container_width=True)
        else:
            st.warning("æš‚æ— æ•°æ®å¯åˆ†æ")
    
    with tab5:
        st.header("âš™ï¸ æ•°æ®åº“ç®¡ç†")
        
        # æ–°å¢åŠŸèƒ½
        st.subheader("ğŸ“ æ–°å¢æ–‡æ¡£")
        
        add_method = st.radio("é€‰æ‹©æ·»åŠ æ–¹å¼", ["æ–‡æœ¬è¾“å…¥", "æ–‡ä»¶ä¸Šä¼ "])
        
        if add_method == "æ–‡æœ¬è¾“å…¥":
            col1, col2 = st.columns([2, 1])
            
            with col1:
                document_text = st.text_area("æ–‡æ¡£å†…å®¹", height=200, placeholder="è¯·è¾“å…¥è¦æ·»åŠ çš„æ–‡æ¡£å†…å®¹...")
            
            with col2:
                document_id = st.text_input("æ–‡æ¡£ID", placeholder="ä¾‹å¦‚ï¼šdoc_001")
                category = st.selectbox("åˆ†ç±»", ["general", "technical", "business", "academic", "other"])
                language = st.selectbox("è¯­è¨€", ["zh", "en", "other"])
                
                if st.button("ğŸ“ æ·»åŠ æ–‡æ¡£", type="primary"):
                    if document_text and document_id:
                        metadata = {
                            "source": "web_text",
                            "category": category,
                            "language": language,
                            "timestamp": datetime.now().isoformat(),
                            "file_name": f"{document_id}.txt"
                        }
                        
                        with st.spinner("æ­£åœ¨æ·»åŠ æ–‡æ¡£..."):
                            if viewer.add_document(document_text, document_id, metadata):
                                st.success(f"âœ… æ–‡æ¡£ {document_id} æ·»åŠ æˆåŠŸï¼")
                                # è§¦å‘åˆ·æ–°
                                st.session_state.refresh_trigger += 1
                                st.rerun()
                    else:
                        st.warning("è¯·å¡«å†™æ–‡æ¡£å†…å®¹å’Œæ–‡æ¡£ID")
        
        else:  # æ–‡ä»¶ä¸Šä¼ 
            uploaded_file = st.file_uploader("é€‰æ‹©æ–‡ä»¶", type=['txt', 'pdf'], help="æ”¯æŒtxtå’Œpdfæ–‡ä»¶")
            
            if uploaded_file is not None:
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.write(f"**æ–‡ä»¶å:** {uploaded_file.name}")
                    st.write(f"**æ–‡ä»¶å¤§å°:** {uploaded_file.size} bytes")
                    
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    if uploaded_file.type == "text/plain":
                        content = uploaded_file.read().decode('utf-8')
                        st.text_area("æ–‡ä»¶å†…å®¹é¢„è§ˆ", content[:500] + "..." if len(content) > 500 else content, height=150)
                    else:
                        st.info("PDFæ–‡ä»¶é¢„è§ˆåŠŸèƒ½å¼€å‘ä¸­...")
                        content = "PDFæ–‡ä»¶å†…å®¹"
                
                with col2:
                    document_id = st.text_input("æ–‡æ¡£ID", value=uploaded_file.name.split('.')[0])
                    category = st.selectbox("åˆ†ç±»", ["general", "technical", "business", "academic", "other"])
                    language = st.selectbox("è¯­è¨€", ["zh", "en", "other"])
                    
                    if st.button("ğŸ“ ä¸Šä¼ æ–‡æ¡£", type="primary"):
                        if document_id:
                            metadata = {
                                "source": "web_upload",
                                "category": category,
                                "language": language,
                                "timestamp": datetime.now().isoformat(),
                                "file_name": uploaded_file.name,
                                "file_size": uploaded_file.size
                            }
                            
                            if uploaded_file.type == "text/plain":
                                with st.spinner("æ­£åœ¨ä¸Šä¼ æ–‡æ¡£..."):
                                    if viewer.add_document(content, document_id, metadata):
                                        st.success(f"âœ… æ–‡æ¡£ {document_id} ä¸Šä¼ æˆåŠŸï¼")
                                        # è§¦å‘åˆ·æ–°
                                        st.session_state.refresh_trigger += 1
                                        st.rerun()
                            else:
                                st.info("PDFæ–‡ä»¶å¤„ç†åŠŸèƒ½å¼€å‘ä¸­ï¼Œè¯·ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶")
                        else:
                            st.warning("è¯·å¡«å†™æ–‡æ¡£ID")
        
        st.markdown("---")
        
        # å¯¼å‡ºåŠŸèƒ½
        st.subheader("ğŸ“¤ å¯¼å‡ºæ•°æ®")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("å¯¼å‡ºä¸ºJSON"):
                try:
                    df = viewer.get_all_documents()
                    if not df.empty:
                        # è½¬æ¢ä¸ºJSONæ ¼å¼
                        json_data = df.to_dict('records')
                        
                        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                        export_data = {
                            "export_time": datetime.now().isoformat(),
                            "database_stats": stats,
                            "documents": json_data
                        }
                        
                        # ä¿å­˜æ–‡ä»¶
                        filename = f"database_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(export_data, f, ensure_ascii=False, indent=2)
                        
                        st.success(f"æ•°æ®å·²å¯¼å‡ºåˆ° {filename}")
                        
                        # æä¾›ä¸‹è½½é“¾æ¥
                        with open(filename, 'r', encoding='utf-8') as f:
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½JSONæ–‡ä»¶",
                                data=f.read(),
                                file_name=filename,
                                mime="application/json"
                            )
                    else:
                        st.warning("æš‚æ— æ•°æ®å¯å¯¼å‡º")
                except Exception as e:
                    st.error(f"å¯¼å‡ºå¤±è´¥: {e}")
        
        with col2:
            if st.button("å¯¼å‡ºä¸ºCSV"):
                try:
                    df = viewer.get_all_documents()
                    if not df.empty:
                        filename = f"database_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                        df.to_csv(filename, index=False, encoding='utf-8-sig')
                        st.success(f"æ•°æ®å·²å¯¼å‡ºåˆ° {filename}")
                        
                        # æä¾›ä¸‹è½½é“¾æ¥
                        with open(filename, 'r', encoding='utf-8-sig') as f:
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½CSVæ–‡ä»¶",
                                data=f.read(),
                                file_name=filename,
                                mime="text/csv"
                            )
                    else:
                        st.warning("æš‚æ— æ•°æ®å¯å¯¼å‡º")
                except Exception as e:
                    st.error(f"å¯¼å‡ºå¤±è´¥: {e}")
        
        # åˆ é™¤åŠŸèƒ½
        st.subheader("ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£")
        
        # åˆå§‹åŒ–åˆ é™¤ç›¸å…³çš„session_state
        if 'delete_confirmed' not in st.session_state:
            st.session_state.delete_confirmed = False
        if 'doc_to_delete' not in st.session_state:
            st.session_state.doc_to_delete = None
        if 'delete_message' not in st.session_state:
            st.session_state.delete_message = ""
        
        if stats.get('document_ids'):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                doc_to_delete = st.selectbox("é€‰æ‹©è¦åˆ é™¤çš„æ–‡æ¡£", stats['document_ids'], key="delete_select")
                
                # æ˜¾ç¤ºè¦åˆ é™¤çš„æ–‡æ¡£ä¿¡æ¯
                if doc_to_delete:
                    doc_info = viewer.get_document_info(doc_to_delete)
                    if doc_info:
                        st.info(f"**æ–‡æ¡£ä¿¡æ¯:** {doc_info['chunks']} ä¸ªå—ï¼Œæ€»é•¿åº¦ {doc_info['total_length']} å­—ç¬¦")
                        st.warning(f"âš ï¸ åˆ é™¤åå°†æ— æ³•æ¢å¤ï¼Œè¯·è°¨æ…æ“ä½œï¼")
            
            with col2:
                # ç¡®è®¤åˆ é™¤å¤é€‰æ¡†
                confirm_delete = st.checkbox("æˆ‘ç¡®è®¤è¦åˆ é™¤è¿™ä¸ªæ–‡æ¡£", key="confirm_delete_checkbox")
                
                # åˆ é™¤æŒ‰é’®
                delete_button = st.button("ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£", type="secondary", help="åˆ é™¤é€‰ä¸­çš„æ–‡æ¡£", key="delete_btn")
                
                # æ˜¾ç¤ºåˆ é™¤æ¶ˆæ¯
                if st.session_state.delete_message:
                    if "æˆåŠŸ" in st.session_state.delete_message:
                        st.success(st.session_state.delete_message)
                    else:
                        st.error(st.session_state.delete_message)
                    # æ¸…é™¤æ¶ˆæ¯
                    st.session_state.delete_message = ""
                
                # å¤„ç†åˆ é™¤æ“ä½œ
                if delete_button:
                    if confirm_delete and doc_to_delete:
                        with st.spinner("æ­£åœ¨åˆ é™¤..."):
                            try:
                                if viewer.delete_document(doc_to_delete):
                                    st.session_state.delete_message = f"âœ… æ–‡æ¡£ {doc_to_delete} å·²æˆåŠŸåˆ é™¤"
                                    # é‡ç½®ç¡®è®¤çŠ¶æ€
                                    st.session_state.delete_confirmed = False
                                    # è§¦å‘åˆ·æ–°
                                    st.session_state.refresh_trigger += 1
                                    st.rerun()
                                else:
                                    st.session_state.delete_message = f"âŒ åˆ é™¤æ–‡æ¡£ {doc_to_delete} å¤±è´¥"
                                    st.rerun()
                            except Exception as e:
                                st.session_state.delete_message = f"âŒ åˆ é™¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
                                st.rerun()
                    else:
                        if not confirm_delete:
                            st.warning("âš ï¸ è¯·å…ˆç¡®è®¤åˆ é™¤æ“ä½œ")
                        else:
                            st.warning("âš ï¸ è¯·é€‰æ‹©è¦åˆ é™¤çš„æ–‡æ¡£")
        else:
            st.info("æš‚æ— æ–‡æ¡£å¯åˆ é™¤")
        
        # æ•°æ®åº“ä¿¡æ¯
        st.subheader("â„¹ï¸ æ•°æ®åº“ä¿¡æ¯")
        st.json(stats)

if __name__ == "__main__":
    main() 