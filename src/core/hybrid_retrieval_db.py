#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºå‘é‡æ•°æ®åº“çš„æ··åˆPDFæ£€ç´¢ç³»ç»Ÿ
ä½¿ç”¨Chromaå‘é‡æ•°æ®åº“æŒä¹…åŒ–å­˜å‚¨æ–‡æ¡£å‘é‡
æ”¯æŒå¤§è§„æ¨¡æ–‡æ¡£æ£€ç´¢å’Œå¢é‡æ›´æ–°
"""

import fitz  # PyMuPDF
import numpy as np
from typing import List, Tuple, Dict, Optional
from test_qwen3_embedding import Qwen3Embedding
from test_qwen3_reranker import Qwen3Reranker
import torch
import re
import os
import json
import time
from datetime import datetime
import chromadb
from chromadb.config import Settings
import uuid

class HybridPDFRetrieverDB:
    def __init__(self, 
                 embedding_model_path: str = "models/Qwen3-Embedding-0.6B/Qwen/Qwen3-Embedding-0.6B",
                 reranker_model_path: str = "models/Qwen3-Reranker-0.6B/Qwen/Qwen3-Reranker-0.6B",
                 db_path: str = "vector_db",
                 collection_name: str = "documents"):
        """
        åˆå§‹åŒ–åŸºäºå‘é‡æ•°æ®åº“çš„æ··åˆPDFæ£€ç´¢å™¨
        Args:
            embedding_model_path: Qwen3 Embeddingæ¨¡å‹è·¯å¾„
            reranker_model_path: Qwen3 Rerankeræ¨¡å‹è·¯å¾„
            db_path: å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„
            collection_name: é›†åˆåç§°
        """
        print("æ­£åœ¨åŠ è½½Qwen3 Embeddingæ¨¡å‹...")
        self.embedding_model = Qwen3Embedding(embedding_model_path)
        print("Embeddingæ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        print("æ­£åœ¨åŠ è½½Qwen3 Rerankeræ¨¡å‹...")
        self.reranker_model = Qwen3Reranker(
            model_name_or_path=reranker_model_path,
            instruction="Given the user query, retrieval the relevant passages",
            max_length=2048
        )
        print("Rerankeræ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        print(f"æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“: {db_path}")
        self.db_path = db_path
        self.collection_name = collection_name
        self._init_vector_db()
        
        self.chunk_size = 300
        self.documents = []
        self.document_metadata = {}
        
    def _init_vector_db(self):
        """åˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“"""
        try:
            # åˆ›å»ºæ•°æ®åº“å®¢æˆ·ç«¯
            self.client = chromadb.PersistentClient(
                path=self.db_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            try:
                self.collection = self.client.get_collection(name=self.collection_name)
                print(f"âœ… æˆåŠŸåŠ è½½ç°æœ‰é›†åˆ: {self.collection_name}")
                print(f"   ç°æœ‰æ–‡æ¡£æ•°é‡: {self.collection.count()}")
            except:
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "Qwen3æ··åˆæ£€ç´¢ç³»ç»Ÿæ–‡æ¡£é›†åˆ"}
                )
                print(f"âœ… æˆåŠŸåˆ›å»ºæ–°é›†åˆ: {self.collection_name}")
                
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def load_pdf(self, pdf_path: str, document_id: Optional[str] = None) -> List[str]:
        """
        åŠ è½½PDFæ–‡ä»¶å¹¶æå–æ–‡æœ¬
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            document_id: æ–‡æ¡£IDï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if document_id is None:
            document_id = str(uuid.uuid4())
            
        print(f"æ­£åœ¨åŠ è½½PDFæ–‡ä»¶: {pdf_path}")
        print(f"æ–‡æ¡£ID: {document_id}")
        
        try:
            doc = fitz.open(pdf_path)
            full_text = ""
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text = page.get_text()
                full_text += f"ç¬¬{page_num + 1}é¡µ: " + text + "\n"
            
            doc.close()
            
            # åˆ†å‰²æ–‡æœ¬
            chunks = self._split_text(full_text)
            
            # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
            self.document_metadata[document_id] = {
                "file_path": pdf_path,
                "file_name": os.path.basename(pdf_path),
                "total_chunks": len(chunks),
                "upload_time": datetime.now().isoformat(),
                "file_size": os.path.getsize(pdf_path)
            }
            
            print(f"æˆåŠŸæå– {len(chunks)} ä¸ªæ–‡æœ¬å—")
            return chunks
            
        except Exception as e:
            print(f"åŠ è½½PDFæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def _split_text(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆå°å—"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def add_documents_to_db(self, documents: List[str], document_id: str, 
                           metadata: Optional[Dict] = None) -> bool:
        """
        å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        Args:
            documents: æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨
            document_id: æ–‡æ¡£ID
            metadata: é¢å¤–å…ƒæ•°æ®
        Returns:
            æ˜¯å¦æˆåŠŸæ·»åŠ 
        """
        if not documents:
            print("æ²¡æœ‰æ–‡æ¡£å¯ä»¥æ·»åŠ ")
            return False
            
        try:
            print(f"æ­£åœ¨å°†æ–‡æ¡£ {document_id} æ·»åŠ åˆ°å‘é‡æ•°æ®åº“...")
            
            # ç”Ÿæˆæ–‡æ¡£å‘é‡
            with torch.inference_mode():
                embeddings = self.embedding_model.encode(documents, is_query=False)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            embeddings_np = embeddings.cpu().detach().numpy()
            
            # ç”Ÿæˆå”¯ä¸€IDåˆ—è¡¨
            ids = [f"{document_id}_chunk_{i}" for i in range(len(documents))]
            
            # å‡†å¤‡å…ƒæ•°æ®
            chunk_metadata = []
            for i, doc in enumerate(documents):
                chunk_meta = {
                    "document_id": document_id,
                    "chunk_index": i,
                    "chunk_length": len(doc),
                    "timestamp": datetime.now().isoformat()
                }
                if metadata:
                    chunk_meta.update(metadata)
                chunk_metadata.append(chunk_meta)
            
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.collection.add(
                embeddings=embeddings_np.tolist(),
                documents=documents,
                metadatas=chunk_metadata,
                ids=ids
            )
            
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“")
            print(f"   å‘é‡ç»´åº¦: {embeddings_np.shape[1]}")
            print(f"   æ€»æ–‡æ¡£æ•°é‡: {self.collection.count()}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def search_similar_documents(self, query: str, top_k: int = 10, 
                                filter_metadata: Optional[Dict] = None) -> List[Dict]:
        """
        åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼æ–‡æ¡£
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            filter_metadata: è¿‡æ»¤æ¡ä»¶
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            with torch.inference_mode():
                query_embedding = self.embedding_model.encode([query], is_query=True)
            
            query_embedding_np = query_embedding.cpu().detach().numpy()
            
            # åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
            results = self.collection.query(
                query_embeddings=query_embedding_np.tolist(),
                n_results=top_k,
                where=filter_metadata
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for i in range(len(results['ids'][0])):
                result = {
                    'id': results['ids'][0][i],
                    'document': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i],
                    'similarity': 1 - results['distances'][0][i]  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                }
                formatted_results.append(result)
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“æœç´¢å¤±è´¥: {e}")
            return []
    
    def hybrid_search_db(self, query: str, top_k_embedding: int = 10, 
                        top_k_final: int = 5, filter_metadata: Optional[Dict] = None) -> List[Dict]:
        """
        åŸºäºå‘é‡æ•°æ®åº“çš„æ··åˆæ£€ç´¢
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k_embedding: Embeddingé˜¶æ®µå€™é€‰æ•°é‡
            top_k_final: æœ€ç»ˆè¿”å›ç»“æœæ•°é‡
            filter_metadata: è¿‡æ»¤æ¡ä»¶
        Returns:
            æœ€ç»ˆç»“æœåˆ—è¡¨
        """
        print(f"æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢: {query}")
        print("="*50)
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå‘é‡æ•°æ®åº“ç²—ç­›
        print("ç¬¬ä¸€é˜¶æ®µï¼šå‘é‡æ•°æ®åº“ç²—ç­›")
        embedding_results = self.search_similar_documents(
            query, top_k_embedding, filter_metadata
        )
        
        if not embedding_results:
            print("âŒ å‘é‡æ•°æ®åº“æœç´¢æ— ç»“æœ")
            return []
        
        print(f"å‘é‡æ•°æ®åº“æ‰¾åˆ° {len(embedding_results)} ä¸ªå€™é€‰æ–‡æ¡£:")
        for i, result in enumerate(embedding_results, 1):
            print(f"  å€™é€‰{i}: ç›¸ä¼¼åº¦={result['similarity']:.4f}, ID={result['id']}")
            print(f"    å†…å®¹: {result['document'][:80]}...")
        
        # ç¬¬äºŒé˜¶æ®µï¼šRerankerç²¾ç­›
        print(f"\nç¬¬äºŒé˜¶æ®µï¼šRerankerç²¾ç­›")
        candidates = [result['document'] for result in embedding_results]
        pairs = [(query, doc) for doc in candidates]
        
        # ä½¿ç”¨Rerankerè®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        reranker_scores = self.reranker_model.compute_scores(pairs)
        
        # ç»„åˆç»“æœ
        final_results = []
        for i, (result, reranker_score) in enumerate(zip(embedding_results, reranker_scores)):
            final_result = {
                'id': result['id'],
                'document': result['document'],
                'metadata': result['metadata'],
                'embedding_similarity': result['similarity'],
                'reranker_score': reranker_score,
                'final_score': reranker_score  # ä½¿ç”¨Rerankeråˆ†æ•°ä½œä¸ºæœ€ç»ˆåˆ†æ•°
            }
            final_results.append(final_result)
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        final_results.sort(key=lambda x: x['final_score'], reverse=True)
        
        print(f"Rerankeré˜¶æ®µé‡æ–°æ’åºç»“æœ:")
        for i, result in enumerate(final_results[:top_k_final], 1):
            print(f"  æ’åº{i}: Rerankeråˆ†æ•°={result['reranker_score']:.4f}, "
                  f"Embeddingç›¸ä¼¼åº¦={result['embedding_similarity']:.4f}, ID={result['id']}")
            print(f"    å†…å®¹: {result['document'][:80]}...")
        
        return final_results[:top_k_final]
    
    def get_database_stats(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            count = self.collection.count()
            
            # è·å–æ‰€æœ‰æ–‡æ¡£çš„å…ƒæ•°æ®
            all_results = self.collection.get()
            
            # ç»Ÿè®¡æ–‡æ¡£ID
            document_ids = set()
            for metadata in all_results['metadatas']:
                if metadata and 'document_id' in metadata:
                    document_ids.add(metadata['document_id'])
            
            stats = {
                'total_chunks': count,
                'unique_documents': len(document_ids),
                'document_ids': list(document_ids),
                'collection_name': self.collection_name,
                'database_path': self.db_path
            }
            
            return stats
            
        except Exception as e:
            print(f"âŒ è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def delete_document(self, document_id: str) -> bool:
        """
        åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰å—
        Args:
            document_id: è¦åˆ é™¤çš„æ–‡æ¡£ID
        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å±äºè¯¥æ–‡æ¡£çš„å—
            results = self.collection.get(
                where={"document_id": document_id}
            )
            
            if not results['ids']:
                print(f"æœªæ‰¾åˆ°æ–‡æ¡£IDä¸º {document_id} çš„æ–‡æ¡£")
                return False
            
            # åˆ é™¤è¿™äº›å—
            self.collection.delete(ids=results['ids'])
            
            print(f"âœ… æˆåŠŸåˆ é™¤æ–‡æ¡£ {document_id} çš„ {len(results['ids'])} ä¸ªå—")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def update_document(self, document_id: str, new_documents: List[str], 
                       new_metadata: Optional[Dict] = None) -> bool:
        """
        æ›´æ–°æŒ‡å®šæ–‡æ¡£
        Args:
            document_id: æ–‡æ¡£ID
            new_documents: æ–°çš„æ–‡æ¡£å†…å®¹
            new_metadata: æ–°çš„å…ƒæ•°æ®
        Returns:
            æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        # å…ˆåˆ é™¤æ—§æ–‡æ¡£
        if not self.delete_document(document_id):
            return False
        
        # æ·»åŠ æ–°æ–‡æ¡£
        return self.add_documents_to_db(new_documents, document_id, new_metadata)

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä½¿ç”¨å‘é‡æ•°æ®åº“çš„æ··åˆæ£€ç´¢ç³»ç»Ÿ"""
    
    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = HybridPDFRetrieverDB()
    
    # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
    stats = retriever.get_database_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # åŠ è½½PDFæ–‡æ¡£
    pdf_path = r"C:\Users\sanrome\Documents\ä¸‰éƒç™½åº•é€šç”¨ç®€å†-zh.pdf"
    document_id = "resume_zhang"
    
    documents = retriever.load_pdf(pdf_path, document_id)
    if not documents:
        print("PDFåŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
    metadata = {
        "source": "pdf",
        "language": "zh",
        "category": "resume"
    }
    
    success = retriever.add_documents_to_db(documents, document_id, metadata)
    if not success:
        print("æ·»åŠ æ–‡æ¡£åˆ°æ•°æ®åº“å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    print("\n" + "="*80)
    print("=== åŸºäºå‘é‡æ•°æ®åº“çš„æ··åˆæ£€ç´¢ç³»ç»Ÿæµ‹è¯• ===")
    print("="*80)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "å¼ ä¸‰çš„å·¥ä½œç»éªŒ",
        "æŠ€æœ¯æŠ€èƒ½å’Œç¼–ç¨‹è¯­è¨€", 
        "åº”è˜çš„å²—ä½å’Œå…¬å¸",
        "æµ‹è¯•å’Œå¼€å‘ç»éªŒ"
    ]
    
    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"æŸ¥è¯¢: {query}")
        print(f"{'='*60}")
        
        # æ··åˆæœç´¢
        results = retriever.hybrid_search_db(query, top_k_embedding=5, top_k_final=3)
        
        print(f"\nğŸ¯ æœ€ç»ˆæœç´¢ç»“æœ:")
        for i, result in enumerate(results, 1):
            print(f"  ç»“æœ{i}: æœ€ç»ˆåˆ†æ•°={result['final_score']:.4f}, "
                  f"Embeddingç›¸ä¼¼åº¦={result['embedding_similarity']:.4f}")
            print(f"    æ–‡æ¡£ID: {result['id']}")
            print(f"    å†…å®¹: {result['document'][:100]}...")
            print(f"    å…ƒæ•°æ®: {result['metadata']}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
    final_stats = retriever.get_database_stats()
    for key, value in final_stats.items():
        print(f"   {key}: {value}")

if __name__ == "__main__":
    main() 