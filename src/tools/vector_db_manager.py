#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡æ•°æ®åº“ç®¡ç†å·¥å…·
æä¾›æ‰¹é‡æ–‡æ¡£ç®¡ç†ã€æ€§èƒ½ç›‘æ§å’Œæ•°æ®åº“ç»´æŠ¤åŠŸèƒ½
"""

import os
import json
import time
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
import glob
from pathlib import Path
import pandas as pd
from hybrid_retrieval_db import HybridPDFRetrieverDB

class VectorDBManager:
    def __init__(self, db_path: str = "vector_db", collection_name: str = "documents"):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“ç®¡ç†å™¨
        Args:
            db_path: å‘é‡æ•°æ®åº“è·¯å¾„
            collection_name: é›†åˆåç§°
        """
        self.db_path = db_path
        self.collection_name = collection_name
        self.retriever = HybridPDFRetrieverDB(db_path=db_path, collection_name=collection_name)
        
    def batch_add_pdfs(self, pdf_directory: str, metadata_template: Optional[Dict] = None) -> Dict:
        """
        æ‰¹é‡æ·»åŠ PDFæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
        Args:
            pdf_directory: PDFæ–‡ä»¶ç›®å½•
            metadata_template: å…ƒæ•°æ®æ¨¡æ¿
        Returns:
            æ‰¹é‡æ“ä½œç»“æœ
        """
        print(f"ğŸ” æ‰«æPDFç›®å½•: {pdf_directory}")
        
        # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = glob.glob(os.path.join(pdf_directory, "*.pdf"))
        pdf_files.extend(glob.glob(os.path.join(pdf_directory, "**/*.pdf"), recursive=True))
        
        if not pdf_files:
            print("âŒ æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return {"success": 0, "failed": 0, "files": []}
        
        print(f"ğŸ“„ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        results = {
            "success": 0,
            "failed": 0,
            "files": [],
            "start_time": datetime.now().isoformat()
        }
        
        for i, pdf_path in enumerate(pdf_files, 1):
            print(f"\n[{i}/{len(pdf_files)}] å¤„ç†æ–‡ä»¶: {os.path.basename(pdf_path)}")
            
            try:
                # ç”Ÿæˆæ–‡æ¡£ID
                file_name = Path(pdf_path).stem
                document_id = f"{file_name}_{int(time.time())}"
                
                # åŠ è½½PDF
                documents = self.retriever.load_pdf(pdf_path, document_id)
                if not documents:
                    print(f"âŒ PDFåŠ è½½å¤±è´¥: {pdf_path}")
                    results["failed"] += 1
                    results["files"].append({
                        "path": pdf_path,
                        "status": "failed",
                        "error": "PDF loading failed"
                    })
                    continue
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    "source": "pdf",
                    "file_path": pdf_path,
                    "file_name": os.path.basename(pdf_path),
                    "file_size": os.path.getsize(pdf_path),
                    "upload_time": datetime.now().isoformat()
                }
                if metadata_template:
                    metadata.update(metadata_template)
                
                # æ·»åŠ åˆ°æ•°æ®åº“
                success = self.retriever.add_documents_to_db(documents, document_id, metadata)
                
                if success:
                    print(f"âœ… æˆåŠŸæ·»åŠ æ–‡æ¡£: {document_id}")
                    results["success"] += 1
                    results["files"].append({
                        "path": pdf_path,
                        "status": "success",
                        "document_id": document_id,
                        "chunks": len(documents)
                    })
                else:
                    print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {document_id}")
                    results["failed"] += 1
                    results["files"].append({
                        "path": pdf_path,
                        "status": "failed",
                        "error": "Database insertion failed"
                    })
                    
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¼‚å¸¸: {e}")
                results["failed"] += 1
                results["files"].append({
                    "path": pdf_path,
                    "status": "failed",
                    "error": str(e)
                })
        
        results["end_time"] = datetime.now().isoformat()
        results["total_time"] = (datetime.fromisoformat(results["end_time"]) - 
                               datetime.fromisoformat(results["start_time"])).total_seconds()
        
        print(f"\nğŸ“Š æ‰¹é‡æ“ä½œå®Œæˆ:")
        print(f"   æˆåŠŸ: {results['success']}")
        print(f"   å¤±è´¥: {results['failed']}")
        print(f"   æ€»è€—æ—¶: {results['total_time']:.2f}ç§’")
        
        return results
    
    def get_detailed_stats(self) -> Dict:
        """è·å–è¯¦ç»†çš„æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.retriever.get_database_stats()
        
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            all_results = self.retriever.collection.get()
            
            # åˆ†ææ–‡æ¡£åˆ†å¸ƒ
            document_stats = {}
            for metadata in all_results['metadatas']:
                if metadata and 'document_id' in metadata:
                    doc_id = metadata['document_id']
                    if doc_id not in document_stats:
                        document_stats[doc_id] = {
                            'chunks': 0,
                            'total_length': 0,
                            'upload_time': metadata.get('timestamp', ''),
                            'source': metadata.get('source', ''),
                            'category': metadata.get('category', '')
                        }
                    document_stats[doc_id]['chunks'] += 1
                    document_stats[doc_id]['total_length'] += metadata.get('chunk_length', 0)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_chunks = len(all_results['ids'])
            avg_chunk_length = sum(meta.get('chunk_length', 0) for meta in all_results['metadatas']) / total_chunks if total_chunks > 0 else 0
            
            detailed_stats = {
                **stats,
                'document_stats': document_stats,
                'avg_chunk_length': avg_chunk_length,
                'total_chunks': total_chunks,
                'database_size_mb': self._get_database_size(),
                'last_updated': datetime.now().isoformat()
            }
            
            return detailed_stats
            
        except Exception as e:
            print(f"âŒ è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return stats
    
    def _get_database_size(self) -> float:
        """è·å–æ•°æ®åº“æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰"""
        try:
            total_size = 0
            for root, dirs, files in os.walk(self.db_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    total_size += os.path.getsize(file_path)
            return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
        except:
            return 0.0
    
    def search_performance_test(self, queries: List[str], iterations: int = 3) -> Dict:
        """
        æœç´¢æ€§èƒ½æµ‹è¯•
        Args:
            queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
            iterations: æµ‹è¯•è¿­ä»£æ¬¡æ•°
        Returns:
            æ€§èƒ½æµ‹è¯•ç»“æœ
        """
        print(f"ğŸš€ å¼€å§‹æœç´¢æ€§èƒ½æµ‹è¯•...")
        print(f"   æŸ¥è¯¢æ•°é‡: {len(queries)}")
        print(f"   è¿­ä»£æ¬¡æ•°: {iterations}")
        
        results = {
            "queries": queries,
            "iterations": iterations,
            "embedding_times": [],
            "reranker_times": [],
            "total_times": [],
            "start_time": datetime.now().isoformat()
        }
        
        for iteration in range(iterations):
            print(f"\nğŸ“Š ç¬¬ {iteration + 1} è½®æµ‹è¯•:")
            
            for i, query in enumerate(queries, 1):
                print(f"   æŸ¥è¯¢ {i}: {query}")
                
                # æµ‹è¯•Embeddingæœç´¢
                start_time = time.time()
                embedding_results = self.retriever.search_similar_documents(query, top_k=10)
                embedding_time = time.time() - start_time
                
                # æµ‹è¯•æ··åˆæœç´¢
                start_time = time.time()
                hybrid_results = self.retriever.hybrid_search_db(query, top_k_embedding=10, top_k_final=5)
                total_time = time.time() - start_time
                
                reranker_time = total_time - embedding_time
                
                results["embedding_times"].append(embedding_time)
                results["reranker_times"].append(reranker_time)
                results["total_times"].append(total_time)
                
                print(f"     Embedding: {embedding_time:.3f}s, Reranker: {reranker_time:.3f}s, æ€»è®¡: {total_time:.3f}s")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        results["avg_embedding_time"] = sum(results["embedding_times"]) / len(results["embedding_times"])
        results["avg_reranker_time"] = sum(results["reranker_times"]) / len(results["reranker_times"])
        results["avg_total_time"] = sum(results["total_times"]) / len(results["total_times"])
        results["end_time"] = datetime.now().isoformat()
        
        print(f"\nğŸ“ˆ æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"   å¹³å‡Embeddingæ—¶é—´: {results['avg_embedding_time']:.3f}s")
        print(f"   å¹³å‡Rerankeræ—¶é—´: {results['avg_reranker_time']:.3f}s")
        print(f"   å¹³å‡æ€»æ—¶é—´: {results['avg_total_time']:.3f}s")
        
        return results
    
    def export_database_info(self, output_file: str = "database_export.json") -> bool:
        """
        å¯¼å‡ºæ•°æ®åº“ä¿¡æ¯åˆ°JSONæ–‡ä»¶
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        Returns:
            æ˜¯å¦æˆåŠŸå¯¼å‡º
        """
        try:
            print(f"ğŸ“¤ å¯¼å‡ºæ•°æ®åº“ä¿¡æ¯åˆ°: {output_file}")
            
            # è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            stats = self.get_detailed_stats()
            
            # è·å–æ‰€æœ‰æ–‡æ¡£å†…å®¹
            all_results = self.retriever.collection.get()
            
            export_data = {
                "export_time": datetime.now().isoformat(),
                "database_stats": stats,
                "documents": []
            }
            
            # æ•´ç†æ–‡æ¡£ä¿¡æ¯
            for i in range(len(all_results['ids'])):
                doc_info = {
                    "id": all_results['ids'][i],
                    "document": all_results['documents'][i],
                    "metadata": all_results['metadatas'][i]
                }
                export_data["documents"].append(doc_info)
            
            # å†™å…¥æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æˆåŠŸå¯¼å‡º {len(export_data['documents'])} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
            return False
    
    def cleanup_old_documents(self, days_old: int = 30) -> Dict:
        """
        æ¸…ç†æ—§æ–‡æ¡£
        Args:
            days_old: åˆ é™¤å¤šå°‘å¤©å‰çš„æ–‡æ¡£
        Returns:
            æ¸…ç†ç»“æœ
        """
        print(f"ğŸ§¹ æ¸…ç† {days_old} å¤©å‰çš„æ–‡æ¡£...")
        
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            all_results = self.retriever.collection.get()
            
            cutoff_date = datetime.now() - timedelta(days=days_old)
            documents_to_delete = []
            
            for i, metadata in enumerate(all_results['metadatas']):
                if metadata and 'timestamp' in metadata:
                    try:
                        doc_date = datetime.fromisoformat(metadata['timestamp'])
                        if doc_date < cutoff_date:
                            documents_to_delete.append(all_results['ids'][i])
                    except:
                        continue
            
            if not documents_to_delete:
                print("æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§æ–‡æ¡£")
                return {"deleted": 0, "total": len(all_results['ids'])}
            
            # åˆ é™¤æ—§æ–‡æ¡£
            self.retriever.collection.delete(ids=documents_to_delete)
            
            print(f"âœ… æˆåŠŸåˆ é™¤ {len(documents_to_delete)} ä¸ªæ—§æ–‡æ¡£")
            
            return {
                "deleted": len(documents_to_delete),
                "total": len(all_results['ids']),
                "deleted_ids": documents_to_delete
            }
            
        except Exception as e:
            print(f"âŒ æ¸…ç†å¤±è´¥: {e}")
            return {"deleted": 0, "error": str(e)}
    
    def generate_report(self, output_file: str = "database_report.html") -> bool:
        """
        ç”Ÿæˆæ•°æ®åº“æŠ¥å‘Š
        Args:
            output_file: è¾“å‡ºHTMLæ–‡ä»¶è·¯å¾„
        Returns:
            æ˜¯å¦æˆåŠŸç”Ÿæˆ
        """
        try:
            print(f"ğŸ“Š ç”Ÿæˆæ•°æ®åº“æŠ¥å‘Š: {output_file}")
            
            stats = self.get_detailed_stats()
            
            # ç”ŸæˆHTMLæŠ¥å‘Š
            html_content = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>å‘é‡æ•°æ®åº“æŠ¥å‘Š</title>
                <meta charset="utf-8">
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; }}
                    .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                    .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }}
                    .stat-card {{ background-color: #fff; border: 1px solid #ddd; padding: 15px; border-radius: 5px; }}
                    .stat-value {{ font-size: 24px; font-weight: bold; color: #007bff; }}
                    .stat-label {{ color: #666; margin-top: 5px; }}
                    table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                    th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    th {{ background-color: #f8f9fa; }}
                </style>
            </head>
            <body>
                <div class="header">
                    <h1>ğŸ” å‘é‡æ•°æ®åº“æŠ¥å‘Š</h1>
                    <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
                
                <div class="stats">
                    <div class="stat-card">
                        <div class="stat-value">{stats.get('total_chunks', 0)}</div>
                        <div class="stat-label">æ€»æ–‡æ¡£å—æ•°</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">{stats.get('unique_documents', 0)}</div>
                        <div class="stat-label">å”¯ä¸€æ–‡æ¡£æ•°</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">{stats.get('database_size_mb', 0):.2f} MB</div>
                        <div class="stat-label">æ•°æ®åº“å¤§å°</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">{stats.get('avg_chunk_length', 0):.0f}</div>
                        <div class="stat-label">å¹³å‡å—é•¿åº¦</div>
                    </div>
                </div>
                
                <h2>ğŸ“‹ æ–‡æ¡£è¯¦æƒ…</h2>
                <table>
                    <tr>
                        <th>æ–‡æ¡£ID</th>
                        <th>å—æ•°</th>
                        <th>æ€»é•¿åº¦</th>
                        <th>æ¥æº</th>
                        <th>åˆ†ç±»</th>
                    </tr>
            """
            
            for doc_id, doc_stats in stats.get('document_stats', {}).items():
                html_content += f"""
                    <tr>
                        <td>{doc_id}</td>
                        <td>{doc_stats['chunks']}</td>
                        <td>{doc_stats['total_length']}</td>
                        <td>{doc_stats['source']}</td>
                        <td>{doc_stats['category']}</td>
                    </tr>
                """
            
            html_content += """
                </table>
            </body>
            </html>
            """
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            print(f"âœ… æˆåŠŸç”ŸæˆæŠ¥å‘Š: {output_file}")
            return True
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå‘é‡æ•°æ®åº“ç®¡ç†åŠŸèƒ½"""
    
    print("=" * 80)
    print("ğŸ”§ å‘é‡æ•°æ®åº“ç®¡ç†å·¥å…·")
    print("=" * 80)
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = VectorDBManager()
    
    # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š å½“å‰æ•°æ®åº“çŠ¶æ€:")
    stats = manager.get_detailed_stats()
    for key, value in stats.items():
        if key != 'document_stats':
            print(f"   {key}: {value}")
    
    # æ€§èƒ½æµ‹è¯•
    print("\nğŸš€ æ€§èƒ½æµ‹è¯•:")
    test_queries = [
        "æœºå™¨å­¦ä¹ ç®—æ³•",
        "æ·±åº¦å­¦ä¹ æŠ€æœ¯",
        "è‡ªç„¶è¯­è¨€å¤„ç†",
        "è®¡ç®—æœºè§†è§‰"
    ]
    
    performance_results = manager.search_performance_test(test_queries, iterations=2)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“Š ç”ŸæˆæŠ¥å‘Š:")
    manager.generate_report("database_report.html")
    
    print("\nâœ… ç®¡ç†å·¥å…·æ¼”ç¤ºå®Œæˆ!")

if __name__ == "__main__":
    main() 