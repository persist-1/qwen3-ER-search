# Ray Serve é…ç½®ä¸ä½¿ç”¨æŒ‡å—

## ğŸš€ Ray ç®€ä»‹

Ray æ˜¯ä¸€ä¸ªå¼€æºçš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œä¸“ä¸ºæœºå™¨å­¦ä¹ å’ŒAIåº”ç”¨è®¾è®¡ã€‚åœ¨ä½ çš„Qwen3æ£€ç´¢ç³»ç»Ÿä¸­ï¼ŒRay Serveç”¨äºéƒ¨ç½²å’Œç®¡ç†AIæ¨¡å‹æœåŠ¡ã€‚

## ğŸ“Š æ ¸å¿ƒæ¦‚å¿µ

### 1. Ray Serve æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HTTP Client   â”‚â”€â”€â”€â–¶â”‚   Ray Serve     â”‚â”€â”€â”€â–¶â”‚   Model Replica â”‚
â”‚                 â”‚    â”‚   Router        â”‚    â”‚   (GPU/CPU)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Model Replica â”‚
                       â”‚   (GPU/CPU)     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. éƒ¨ç½²é…ç½®å‚æ•°

```python
@serve.deployment(
    num_replicas=NUM_REPLICAS,           # å‰¯æœ¬æ•°é‡
    ray_actor_options={
        "num_gpus": NUM_GPUS,           # GPUæ•°é‡
        "num_cpus": 1,                  # CPUæ ¸å¿ƒæ•°
        "memory": 2000 * 1024 * 1024,   # å†…å­˜é™åˆ¶ (2GB)
    }
)
```

## ğŸ¯ é…ç½®ä¼˜åŒ–ç­–ç•¥

### 1. GPUèµ„æºåˆ†é…

#### å•GPUç¯å¢ƒ
```python
NUM_REPLICAS = 2
NUM_GPUS = 0.5  # æ¯ä¸ªå‰¯æœ¬å ç”¨50% GPU
# ç»“æœï¼šä¸¤ä¸ªå‰¯æœ¬å…±äº«ä¸€ä¸ªGPU
```

#### å¤šGPUç¯å¢ƒ
```python
NUM_REPLICAS = 4
NUM_GPUS = 0.5  # æ¯ä¸ªå‰¯æœ¬å ç”¨50% GPU
# ç»“æœï¼š4ä¸ªå‰¯æœ¬åˆ†å¸ƒåœ¨2ä¸ªGPUä¸Š
```

#### é«˜è´Ÿè½½ç¯å¢ƒ
```python
NUM_REPLICAS = 6
NUM_GPUS = 0.3  # æ¯ä¸ªå‰¯æœ¬å ç”¨30% GPU
# ç»“æœï¼š6ä¸ªå‰¯æœ¬åˆ†å¸ƒåœ¨2ä¸ªGPUä¸Šï¼Œæœ‰è´Ÿè½½å‡è¡¡
```

### 2. å†…å­˜ä¼˜åŒ–

```python
@serve.deployment(
    num_replicas=2,
    ray_actor_options={
        "num_gpus": 0.5,
        "memory": 4000 * 1024 * 1024,  # 4GBå†…å­˜
        "object_store_memory": 1000 * 1024 * 1024,  # 1GBå¯¹è±¡å­˜å‚¨
    }
)
```

### 3. ç½‘ç»œé…ç½®

```python
serve.start(
    http_options=HTTPOptions(
        host="0.0.0.0",           # ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£
        port=4008,                # æœåŠ¡ç«¯å£
        root_path="/api/v1",      # APIæ ¹è·¯å¾„
        max_concurrent_queries=100,  # æœ€å¤§å¹¶å‘æŸ¥è¯¢æ•°
    )
)
```

## ğŸ”§ æ€§èƒ½è°ƒä¼˜

### 1. æ‰¹å¤„ç†ä¼˜åŒ–
```python
@app.post("/embedding/api")
def embedding(self, texts: EmbeddingInput):
    # æ‰¹å¤„ç†å¤šä¸ªæ–‡æœ¬
    batch_size = 32
    results = []
    
    for i in range(0, len(texts.input), batch_size):
        batch = texts.input[i:i+batch_size]
        with torch.inference_mode():
            output = self.emodel_embedding.encode(batch, is_query=texts.is_query)
            results.extend(output.cpu().detach().numpy().tolist())
    
    return results
```

### 2. ç¼“å­˜æœºåˆ¶
```python
from functools import lru_cache

class BatchCombineInferModel:
    def __init__(self, ...):
        self._cache = {}
    
    @lru_cache(maxsize=1000)
    def cached_encode(self, text: str, is_query: bool):
        return self.emodel_embedding.encode([text], is_query=is_query)
```

### 3. å¼‚æ­¥å¤„ç†
```python
import asyncio

@app.post("/embedding/api")
async def embedding_async(self, texts: EmbeddingInput):
    # å¼‚æ­¥å¤„ç†è¯·æ±‚
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        None, 
        self.emodel_embedding.encode, 
        texts.input, 
        texts.is_query
    )
    return result.cpu().detach().numpy().tolist()
```

## ğŸ“ˆ ç›‘æ§å’Œæ—¥å¿—

### 1. æ€§èƒ½ç›‘æ§
```python
import time
from ray.serve import metrics

@app.post("/embedding/api")
def embedding(self, texts: EmbeddingInput):
    start_time = time.time()
    
    # å¤„ç†è¯·æ±‚
    result = self.emodel_embedding.encode(texts.input, is_query=texts.is_query)
    
    # è®°å½•æŒ‡æ ‡
    processing_time = time.time() - start_time
    metrics.record_metric("embedding_latency", processing_time)
    metrics.record_metric("embedding_requests", 1)
    
    return result.cpu().detach().numpy().tolist()
```

### 2. å¥åº·æ£€æŸ¥
```python
@app.get("/health")
def health_check(self):
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "gpu_usage": torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated(),
        "model_loaded": hasattr(self, 'emodel_embedding')
    }
```

## ğŸš€ éƒ¨ç½²æœ€ä½³å®è·µ

### 1. ç”Ÿäº§ç¯å¢ƒé…ç½®
```python
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
PRODUCTION_CONFIG = {
    "num_replicas": 4,           # 4ä¸ªå‰¯æœ¬
    "num_gpus": 0.25,           # æ¯ä¸ªå‰¯æœ¬25% GPU
    "num_cpus": 2,              # 2ä¸ªCPUæ ¸å¿ƒ
    "memory": 8000 * 1024 * 1024,  # 8GBå†…å­˜
    "max_concurrent_queries": 50,   # æœ€å¤§å¹¶å‘50
}
```

### 2. å¼€å‘ç¯å¢ƒé…ç½®
```python
# å¼€å‘ç¯å¢ƒæ¨èé…ç½®
DEV_CONFIG = {
    "num_replicas": 1,           # 1ä¸ªå‰¯æœ¬
    "num_gpus": 0.5,            # 50% GPU
    "num_cpus": 1,              # 1ä¸ªCPUæ ¸å¿ƒ
    "memory": 4000 * 1024 * 1024,  # 4GBå†…å­˜
    "max_concurrent_queries": 10,   # æœ€å¤§å¹¶å‘10
}
```

### 3. è´Ÿè½½å‡è¡¡
```python
# è‡ªå®šä¹‰è´Ÿè½½å‡è¡¡ç­–ç•¥
@serve.deployment(
    num_replicas=3,
    ray_actor_options={"num_gpus": 0.3}
)
class LoadBalancedModel:
    def __init__(self):
        self.request_count = 0
    
    @app.post("/api")
    def handle_request(self, request):
        self.request_count += 1
        # æ ¹æ®è´Ÿè½½é€‰æ‹©å¤„ç†ç­–ç•¥
        if self.request_count > 100:
            # é«˜è´Ÿè½½æ—¶çš„å¤„ç†é€»è¾‘
            pass
        return self.process_request(request)
```

## ğŸ” æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

#### GPUå†…å­˜ä¸è¶³
```python
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘GPUä½¿ç”¨é‡
NUM_GPUS = 0.3  # ä»0.9å‡å°‘åˆ°0.3
```

#### å†…å­˜æ³„æ¼
```python
# è§£å†³æ–¹æ¡ˆï¼šå®šæœŸæ¸…ç†ç¼“å­˜
import gc

@app.post("/api")
def api_with_cleanup(self, request):
    result = self.process(request)
    gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
    return result
```

#### ç½‘ç»œè¶…æ—¶
```python
# è§£å†³æ–¹æ¡ˆï¼šå¢åŠ è¶…æ—¶æ—¶é—´
serve.start(
    http_options=HTTPOptions(
        host="0.0.0.0",
        port=4008,
        request_timeout_s=300,  # 5åˆ†é’Ÿè¶…æ—¶
    )
)
```

### 2. è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ·»åŠ è°ƒè¯•ç«¯ç‚¹
@app.get("/debug")
def debug_info(self):
    return {
        "gpu_memory": torch.cuda.memory_allocated(),
        "cpu_memory": psutil.virtual_memory().used,
        "active_requests": len(self._active_requests),
        "model_status": "loaded" if hasattr(self, 'model') else "not_loaded"
    }
```

## ğŸ“š æ‰©å±•é˜…è¯»

- [Rayå®˜æ–¹æ–‡æ¡£](https://docs.ray.io/)
- [Ray Serveæ•™ç¨‹](https://docs.ray.io/en/latest/serve/index.html)
- [Rayæ€§èƒ½è°ƒä¼˜æŒ‡å—](https://docs.ray.io/en/latest/ray-core/performance/index.html)
- [Rayæ•…éšœæ’é™¤](https://docs.ray.io/en/latest/ray-core/troubleshooting/index.html) 